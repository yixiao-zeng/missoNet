---
title: "An Introduction to missoNet"
author:
  - Yixiao Zeng
date: "`r format(Sys.time(), '%B %d, %Y')`"
output: rmarkdown::html_vignette
bibliography: missoNet_refs.bib
nocite: '@*'
vignette: >
  %\VignetteIndexEntry{An Introduction to missoNet}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

## Introduction

**missoNet** is a package that fits multi-task regression and estimates the conditional network structure among multiple correlated outputs (response variables) via penalized maximum likelihood. **missoNet** allows missing data in the output matrix, it enjoys the theoretical and computational benefits of convexity and returns solutions that are comparable/close to the clean data estimates.  

**missoNet** assumes the model
$$
\mathbf{Y} = \mathbf{X}\mathbf{B}^* + \mathbf{E},\ \ \mathbf{E}_{i\cdot} \stackrel{iid}{\sim} \mathcal{MVN}(\mathbf{0},(\mathbf{\Theta}^*)^{-1})\ \ \forall i = 1,...,n,
$$
where $\mathbf{Y} \in \mathbb{R}^{n\times q}$ and $\mathbf{X} \in \mathbb{R}^{n\times p}$ denotes the centered output (response) matrix and input (predictor) matrix, respectively. $\mathbf{E}_{i\cdot} \in \mathbb{R}^{q}$ is the $i$-th row of the error matrix. $n, p, q$ denotes the sample size, the number of predictors and the number of responses. The regression coefficient matrix $\mathbf{B}^* \in \mathbb{R}^{p\times q}$ and the precision (inverse covariance) matrix $\mathbf{\Theta}^* \in \mathbb{R}^{q\times q}$ are parameters to be estimated. Note that the entries of $\mathbf{\Theta}^*$ have a one-to-one correspondence with partial correlations. That is, the random variables $Y_j$ and $Y_k$ are conditionally independent given $X$ and all other remaining variables in $Y$ iff the corresponding partial correlation coefficient is zero.

This method solves a penalized MLE problem
$$
(\hat{\mathbf{\Theta}},\hat{\mathbf{B}}) = \underset{\mathbf{\Theta} \succeq 0,\ \mathbf{B}}{\text{argmin}}\ 
\mathrm{tr}\left[\frac{1}{n}(\mathbf{Y} - \mathbf{XB})^\top(\mathbf{Y} - \mathbf{XB}) \mathbf{\Theta}\right] 
- \mathrm{log}|\mathbf{\Theta}| + \lambda_{\Theta}(\|\mathbf{\Theta}\|_{1,\mathrm{off}} + 1_{n\leq \mathrm{max}(p,q)} \|\mathbf{\Theta}\|_{1,\mathrm{diag}}) + \lambda_{B}\|\mathbf{B}\|_1.
$$
$\mathbf{B}^*$ that mapping inputs to outputs and $\mathbf{\Theta}^*$ that revealing the outputs' conditional dependencies are estimated for the lasso penalty -- $\lambda_B$ and $\lambda_\Theta$, over a grid of values (on the log scale) covering the entire range of possible solutions. To learn more about sparse multi-task regression with inverse covariance estimation using clean data, see [MRCE](https://cran.r-project.org/web/packages/MRCE/index.html).

However, missingness in real data is inevitable, many standard methods of multi-task learning either assume the data is fully-observed (eg. MRCE), or dealing with missing data through some likelihood-based methods such as EM algorithm (e.g, [cglasso](https://cran.r-project.org/web/packages/cglasso/index.html)). A Challenge in this context is the possible non-convexity of associated optimization problem with missing data, as well as the high computational cost from iteratively updating parameters. 

**missoNet** aims to handle a specific class of datasets that the output matrix $\mathbf{Y}$ contains missing data which is missing at random (MAR). To use **missoNet**, users do not need to possess additional knowledge for pre-processing missing data (e.g., imputation) nor selections of regularization parameters, the method provides an unified framework for automatically solving a convex modification of the multi-task learning problem defined above using corrupted datasets.

The package includes methods for cross-validation, and functions for prediction and plotting. It has function arguments in the same style as those of **glmnet**, making it easy for experienced users to get started with.


## Installation

To install the package **missoNet** from CRAN, type the following command in R console:
```{r, echo = TRUE, eval = FALSE}
install.packages("missoNet")
```
Or install the development version of **missoNet** from GitHub:
```{r, echo = TRUE, eval = FALSE}
if(!require("devtools")) {
  install.packages("devtools")
}
devtools::install_github("yixiao-zeng/missoNet")
```


## Quick Start

The purpose of this section is to give users a general overview of the functions and their usages. We will briefly go over the main functions, basic operations and outputs.

First, we load the **missoNet** package:
```{r}
library(missoNet)
```

We will use a simulated dataset for illustration. To generate a set of data containing missing values, we can simply call the built-in function `generateData`:
```{r}
## specify a random seed for reproducibility
## the missing mechanism can also be "MCAR" or "MNAR"
sim.dat <- generateData(n = 150, p = 15, q = 12, rho = 0.1, missing.type = "MAR", with.seed = 1512)

tr <- 1:120  ## training set
va <- 121:150  ## validation set
```

This command returns an object containing all necessary components for analysis including: 

* `X`, `Y`: An input matrix $\mathbf{X} \in \mathbb{R}^{n\times p}$ and a clean output matrix $\mathbf{Y} \in \mathbb{R}^{n\times q}$, in which rows correspond to samples and columns correspond to variables. 

* `Beta`, `Theta`: $\mathbf{Y}$ is sampled from $\mathcal{MVN}(\mathbf{X}\mathbf{B}^*,(\mathbf{\Theta}^*)^{-1})$, where $\mathbf{B}^* \in \mathbb{R}^{p\times q}$ and $\mathbf{\Theta}^* \in \mathbb{R}^{q\times q}$ are the generated true parameters and will be estimated (see the later section for more details of their sparse structures). 

* `Z`: A corrupted output matrix $\mathbf{Z} \in \mathbb{R}^{n\times q}$ is defined as: for all $i=1,\dots,n$ and $j=1,\dots,q$, $\mathbf{Z}_{ij}=$`NA` if $\mathbf{M}_{ij}=1$, otherwise $\mathbf{Z}_{ij}=\mathbf{Y}_{ij}$, where $\mathbf{M} \in \mathbb{R}^{n\times q}$ is an indicator matrix of missingness. 

Column-wise (or global) missing probability and missing mechanism are specified by `rho` and `missing.type`, respectively. `rho` can be a scalar or a vector of length $q$, here `rho = 0.1` indicates an overall missing rate of around 10%. 

__Note 1__: the program only accepts missing values that are coded as either `NA`s or `NaN`s.

__Note 2__: although the program can make an estimate under $n \leq \text{max}(p,q)$, it is likely to result in a slow convergence rate and excessive estimation variance. Therefore, we suggest that (both predictor and response) variables can be filtered based on some of their properties before any model is fitted. For example in genomics, where the $\mathbf{X}$ matrix can be very wide, we often filter features based on variance.

We can easily visualize how missing data distributes in the corrupted output matrix $\mathbf{Z}$ using package **visdat**:
```{r, eval = FALSE}
Z <- sim.dat$Z
visdat::vis_miss(as.data.frame(Z))
```
```{r, include = FALSE, eval = FALSE}
png(file = "vismis.png", width = 1900, height = 1300, res = 260)
suppressWarnings(visdat::vis_miss(as.data.frame(sim.dat$Z)))
dev.off()
```
```{r, echo = FALSE, fig.align = 'left', out.width = "80%"}
knitr::include_graphics(system.file("extdata", "vismis.png", package = "missoNet"))
```

A single model can be fitted using the most basic call to `missoNet`:
```{r}
## using the training set to fit the model
## `lambda.Beta` and `lambda.Theta` are arbitrarily set to 0.1
## `verbose = 0` suppresses printing messages
X.tr <- sim.dat$X[tr, ]
Z.tr <- sim.dat$Z[tr, ]
fit <- missoNet(X = X.tr, Y = Z.tr, lambda.Beta = 0.1, lambda.Theta = 0.1, verbose = 0)
```

Or more commonly used by fitting with a grid of values (numeric vector) for $\lambda_B$ and $\lambda_\Theta$, which have __one-to-one correspondence__:
```{r}
lambda.Beta.vec <- 10^(seq(from = 0, to = -1, length.out = 10))  ## 10 values on the log scale, from 1 to 0.1
lambda.Theta.vec <- rep(0.1, 10)  ## for each value of lambda.Beta, lambda.Theta remains constant at 0.1
fit_list <- missoNet(X = X.tr, Y = Z.tr, lambda.Beta = lambda.Beta.vec, lambda.Theta = lambda.Theta.vec, verbose = 0)
```

The command above returns a sequence of models for users to choose from. In many cases, users may prefer the software to select one of them. Cross-validation is perhaps the simplest and most widely used method for this task. `cv.missoNet` is the main function to do cross-validation, along with supporting methods such as `plot` and `predict`.

Here we use `cv.missoNet` to do a five-fold cross-validation (CV), samples will be permuted before splitting into multiple folds. For reproducibility, we assign a random seed to the permutation.
```{r}
cvfit <- cv.missoNet(X = X.tr, Y = Z.tr, kfold = 5,
                     lambda.Beta = lambda.Beta.vec, lambda.Theta = lambda.Theta.vec,
                     permute = TRUE, with.seed = 433, verbose = 0)
```

The program has warned us that the range of $\lambda_\Theta$ is inappropriate, we need to supply a sequence of $\lambda_\Theta$ covering larger values. However, picking a suitable range of such hyperparameters may require experience or trial and error, the program has provided us with a simpler way to solve this problem. 

Let's fit the cross-validation model again, this time all folds run in parallel on two CPU cores. Parallelization of **missoNet** relies on package **snowfall**, so make sure load the package beforehand.
```{r, eval = FALSE}
library(snowfall, quietly = TRUE)
cvfit <- cv.missoNet(X = X.tr, Y = Z.tr, kfold = 5,
                     fit.1se = TRUE, parallel = TRUE, cpus = 2,
                     permute = TRUE, with.seed = 433, verbose = 0)
```

Note that we do not explicitly specify the regularization parameters $\lambda_B$ and $\lambda_\Theta$. In this case, a grid of $\lambda_B$ and $\lambda_\Theta$ values in a (hopefully) reasonable range will be computed and used by the program. Users can also choose to provide one of them such that the program will compute the other one. Generally, it is difficult at the beginning to feed suitable $\lambda$ sequences, so we __suggest__ users start analysis using `cv.missoNet`, and let the program compute appropriate $\lambda_B$s and $\lambda_\Theta$s itself then pick the optimal combination of them at which the smallest mean CV error is achieved.

`cv.missoNet` returns a `cv.missoNet` object, a list with all the ingredients of the cross-validated fit. We can execute the plot method
```{r, eval = FALSE}
## The exact values of mean CV errors along with upper and lower standard deviation bounds 
## can be accessed via `cvfit$cvm`, `cvfit$cvup` and `cvfit$cvlo`, respectively.
plot(cvfit)
```
```{r, echo = FALSE, fig.align = 'left', out.width = "80%"}
knitr::include_graphics(system.file("extdata", "cvfitHeat.png", package = "missoNet"))
```

to visualize mean CV error in a heatmap style. The white solid box marks out the position of the smallest mean CV error with corresponding $\lambda_B$ and $\lambda_\Theta$ (`lambda.min`), and the white dashed boxes indicate the largest $\lambda_B$ and $\lambda_\Theta$ at which the mean CV error is within one standard error of the minimum (`lambda.Beta.1se` and `lambda.Theta.1se`), by fixing the other one at `lambda.min`. We often use this "one-standard-error" rule when selecting the most parsimonious model; this acknowledges the fact that the CV surface is estimated with error, so error on the side of parsimony is preferable.

We can also plot it in the form of scatter diagram:
```{r, eval = FALSE}
plot(cvfit, type = "cv.scatter")
```
```{r, echo = FALSE, fig.align = 'left', out.width = "80%"}
knitr::include_graphics(system.file("extdata", "cvfitScat.png", package = "missoNet"))
```

After cross-validation, $\mathbf{B}^*$ and $\mathbf{\Theta}^*$ can be estimated at these three special $\lambda$ values -- `lambda.min`, `lambda.Beta.1se` and `lambda.Theta.1se`, the corresponding results along with the exact $\lambda$ values are stored in separate lists: `est.min`, `estB.1se` and `estTht.1se` (`estB.1se` and `estTht.1se` are not `NULL` only if the argument `fit.1se = TRUE` when calling `cv.missoNet`).

Let's extract the estimates of model parameters at `lambda.min` and `lambda.Beta.1se` then plot them aside the ground truth $\mathbf{B}^*$ and $\mathbf{\Theta}^*$:
```{r, eval = FALSE}
## define a plotting function
plot_heatmap <- function(est, col, legend = FALSE, lgd_name, title) {
  return(ComplexHeatmap::Heatmap(est, cluster_rows = FALSE, cluster_columns = FALSE,
                                 col = col, show_heatmap_legend = legend, name = lgd_name,
                                 column_names_gp = grid::gpar(fontsize = 8),
                                 row_names_gp = grid::gpar(fontsize = 8), row_names_side = "left", 
                                 border = TRUE, column_title = title))
}

## color space
col <- circlize::colorRamp2(c(-2, 0, 2), c("blue", "white", "red"))

## Beta*
Beta_star <- sim.dat$Beta
Beta_star_ht <- plot_heatmap(Beta_star, col, title = expression(paste(bold(Beta), "*")))

## Beta_hat at `lambda.min`
Beta_hat_min <- cvfit$est.min$Beta
Beta_hat_min_ht <- plot_heatmap(Beta_hat_min, col, title = expression(paste(hat(bold(Beta)), " at `lambda.min`")))

## Beta_hat at `lambda.Beta.1se`
Beta_hat_1se <- cvfit$estB.1se$Beta
Beta_hat_1se_ht <- plot_heatmap(Beta_hat_1se, col, legend = TRUE, lgd_name = "values",
                                title = expression(paste(hat(bold(Beta)), " at `lambda.Beta.1se`")))

## Theta*
Theta_star <- sim.dat$Theta
Theta_star_ht <- plot_heatmap(Theta_star, col, title = expression(paste(bold(Theta), "*")))

## Theta_hat at `lambda.min`
Theta_hat_min <- cvfit$est.min$Theta
Theta_hat_min_ht <- plot_heatmap(Theta_hat_min, col, title = expression(paste(hat(bold(Theta)), " at `lambda.min`")))
  
## Theta_hat at `lambda.Beta.1se`
Theta_hat_1se <- cvfit$estB.1se$Theta
Theta_hat_1se_ht <- plot_heatmap(Theta_hat_1se, col, legend = TRUE, lgd_name = "values",
                                 title =expression(paste(hat(bold(Theta)), " at `lambda.Beta.1se`")))

## plot
Beta_star_ht + Beta_hat_min_ht + Beta_hat_1se_ht
Theta_star_ht + Theta_hat_min_ht + Theta_hat_1se_ht
```
```{r, echo = FALSE, fig.align = 'left', out.width = "100%"}
knitr::include_graphics(system.file("extdata", "cvfitEst1.png", package = "missoNet"))
```
```{r, echo = FALSE, fig.align = 'left', out.width = "100%"}
knitr::include_graphics(system.file("extdata", "cvfitEst2.png", package = "missoNet"))
```

Like other regression methods, predictions can be made based on the fitted `cv.missoNet` object as well. The code below gives prediction for a new input matrix `newx` at `lambda.min` (`s = "lambda.Beta.1se"` and `s = "lambda.Theta.1se"` is available only when `fit.1se = TRUE`):
```{r, eval = FALSE}
## prediction on the validation set
## `s` can also be "lambda.Beta.1se" or "lambda.Theta.1se"
## newy = mu + newx %*% Beta
newy <- predict(cvfit, newx = sim.dat$X[va, ], s = "lambda.min")

cat("dim(newy):", dim(newy))
```
```{r, echo = FALSE}
cat("dim(newy): 30 12")
```

Users now should be able to fit **missoNet** models with the functions introduced so far. There are many more arguments in the package that give users a great deal of flexibility. To learn more, move on to the next section. In the section for real data application, we demonstrate a complete process of analyzing a neuroimaging and genetic dataset using **missoNet**, we __highly recommend__ interested users to browse this content.


## Other Commonly Used Function Arguments

**missoNet** provides other arguments for users to customize the fit, we introduce some commonly used arguments here (check the help pages for more information).

### `cv.missoNet`

* `rho`: a scalar or numeric vector for the missing probability of response variables. Default is `rho = NULL` and the program will compute the empirical missing rates for columns of `Y` and use them as the working missing probability; the user-supplied value overrides this.

* `lambda.Beta` and `lambda.Theta` are optional user-supplied sequences of $\lambda$ values (will be automatically arranged in a descending order internally). When $\lambda$s are automatically generated (`lambda.Beta/lambda.Theta = NULL`), the sequences can be controlled by the following arguments:
    + `lamBeta.min.ratio`/`lamTheta.min.ratio`: the smallest value of $\lambda$ will be the data-derived `lambda.max` multiplied by this positive ratio. This argument controls the width of the generated $\lambda$ sequence by trimming its smallest end. The default depends on the sample size $n$ relative to the number of predictors $p$/responses $q$. If $n>p$/$n>q$, the default is 1.0E-4 otherwise it is 1.0E-2. A very small value of `lamBeta.min.ratio`/`lamTheta.min.ratio` may significantly increase the runtime and lead to a saturated fit when $n \leq p$/$n \leq q$.
    + `n.lamBeta`/`n.lamTheta`: the program generates `n.lamBeta`/`n.lamTheta` values linear on the log scale from the largest down to the smallest. If $n>p$/$n>q$, the default is 40 otherwise it is 20. Try to avoid directly supplying a very large number since the program will fit `n.lamBeta * n.lamTheta` models in total for each fold of cross-validation. Instead, users can adopt a strategy of "coarse + fine" search, see the next section for an example.
   + `lamBeta.scale.factor`/`lamTheta.scale.factor`: a positive multiplication factor controlling the overall magnitudes of the entire $\lambda$ sequence. This argument plays the role of shifting the $\lambda$ sequence on the log scale, the default is `1` and a typical usage is when the program warns that the generated $\lambda$ sequence has an inappropriate range (too large or too small), so that the optimal selection of $\lambda$ from cross-validation is close to the boundaries of the search range.

* `standardize` and `standardize.response` are logical flags for standardization of variables in $\mathbf{X}$ and $\mathbf{Y}$ to have unit variance (the default) prior to fitting the model. The estimated parameters are always returned on the original scale. **missoNet** computes appropriate $\lambda$ sequences relying on standardization. If users wish to compare the results with those of other softwares, it is suggested to supply a dataset standardized beforehand (using `scale()` or similar functions) then set `standardize` and `standardize.response` to `FALSE`.

* `fit.1se` is a logical flag telling the program should $\mathbf{B}^*$ and $\mathbf{\Theta}^*$ be estimated at the largest `lambda.Beta` and `lambda.Theta` respectively according to the one-standard-error rule? Default is `fit.1se = FALSE`, then `estB.1se` and `estTht.1se` in the returned object will be `NULL`.

* `fit.relax`: if `TRUE`, the program will have an additional debiased estimate of the network structure by re-estimating the edges in the active set (non-zero off-diagonal elements) of $\hat{\mathbf{\Theta}}$ without penalization (`lambda.Theta = 0`), which could be useful for some conditional inter-dependencies analyses. This "relaxed" fit is stored in `relax.net`. WARNING: there may be convergence issues if the working empirical covariance matrix is not of full rank (e.g., $n < q$).

* `verbose`: can be 0, 1 or 2. `verbose = 0` -- silent; `verbose = 1` -- limited tracing; `verbose = 2` -- detailed tracing. Default is `verbose = 1`, set to `2` if you wish to track algorithm convergence.


### `generateData`

`generateData` provides a convenient way for users to quickly generate simulation data and get started to build their own analysis frameworks.

Given the predictor matrix $\mathbf{X}$ and the true parameters $\mathbf{B}^*$ and $\mathbf{\Theta}^*$, a clean response matrix $\mathbf{Y}$ is generated by sampling from $\mathcal{MVN}(\mathbf{X}\mathbf{B}^*,(\mathbf{\Theta}^*)^{-1})$. A matched missing-data version $\mathbf{Z}$ has elements $\mathbf{Z}_{ij} = 1_{\mathbf{M}_{ij}=0}\mathbf{Y}_{ij}$, where $\mathbf{M}$ is a indicator matrix of missingness parameterized by missing proportion `rho` and missing mechanism `missing.type`. Users can also directly provide the error matrix $\mathbf{E}$, then the generating process will be simplified to $\mathbf{Y} = \mathbf{X}\mathbf{B}^* + \mathbf{E}$, meanwhile the argument `Theta` (for $\mathbf{\Theta}^*$) is ignored.

* `Beta`: (optional) a user-supplied true regression coefficient matrix $\mathbf{B}^*$ ($p \times q$). If `Beta = NULL` (the default), a dense matrix will first be created by setting each element to a random draw from `rnorm(0, 1)`, then a sparse $\mathbf{B}^*$ is generated by randomly assigning zeros to some elements in this matrix. There are two kinds of sparsity controlled by arguments `Beta.row.sparsity` and `Beta.elem.sparsity`:
    + `Beta.row.sparsity`: a Bernoulli parameter between 0 and 1 controlling the approximate proportion of non-zero rows in $\mathbf{B}^*$.
    + `Beta.elem.sparsity`: a Bernoulli parameter between 0 and 1 controlling the approximate proportion of non-zero elements in those non-zero rows (i.e., rows that are not all zeros).

* `Theta`: (optional) a user-supplied (positive definite) true precision matrix $\mathbf{\Theta}^*$ ($q \times q$). Default is `Theta = NULL` and the program will generate a block-structured $\mathbf{\Theta}^*$ having four blocks corresponding to four types of network structures: independent, weak graph, strong graph and chain. Only needed when `E = NULL`.

* `Sigma.X`: (optional) a user-supplied (positive definite) covariance matrix ($p \times p$) for generating the input matrix $\mathbf{X} \sim \mathcal{MVN}(\boldsymbol{0}, \mathbf{\Sigma}_X)$. If `Sigma.X = NULL` (the default), generate $\mathbf{X}$ using an AR(1) covariance with 0.7 autocorrelation. Only needed when `X = NULL`.

* `rho`: a scalar or a vector of length $q$ specifying the approximate proportion of missing values in each column of $\mathbf{Z}$. Note that a scalar will be automatically converted to a vector filled with same values.

* `missing.type`: this argument determines how the corrupted output matrix $\mathbf{Z}$ is generated based on $\mathbf{Y}$, can be one of `"MCAR"`, `"MAR"` and `"MNAR"`:
    + `"MCAR"`: missing completely at random. For all $i=1,\dots,n$ and $j=1,\dots,q$: $\mathbf{Y}_{ij}$ is missing if $\mathbf{M}_{ij}=1$, where $\mathbf{M}_{ij}$ are i.i.d. Bernoulli draws with probability `rho[j]`;
    + `"MAR"`: missing at random. For all $i=1,\dots,n$ and $j=1,\dots,q$: $\mathbf{Y}_{ij}$ is missing if $\mathbf{M}_{ij}=1$, where $\mathbf{M}_{ij}$ are random Bernoulli draws with probability `rho[j]*2*`$\frac{1}{1+e^{-[\mathbf{X}\mathbf{B}^*]_{ij}}}$;
    + `"MNAR"`: missing not at random. For all $i=1,\dots,n$ and $j=1,\dots,q$: $\mathbf{Y}_{ij}$ is missing if $\mathbf{M}_{ij}=1$, where $\mathbf{M}_{ij}=1_{\mathbf{Y}_{ij} < T^{j}}(\mathbf{Y}_{ij})$, in which $T^{j} = Q$(`rho[j]`) is the `rho[j]`-quantile of $\mathbf{Y}_{\cdot j}$.

<ul>
Imagine a dummy variable $\text{miss}_{(Y)}$. Under MCAR, $\text{miss}_{(Y)}$ is not related to $Y$ or to $X$; under MAR, $\text{miss}_{(Y)}$ is related to $X$, but not related to $Y$ after $X$ is controlled; under MNAR, $\text{miss}_{(Y)}$ is related to $Y$ itself, even after $X$ is controlled.
</ul>


### `plot`

* `detailed.axes` is a logical flag telling the plotting function should detailed axes be plotted? Default is `detailed.axes = TRUE`, set to `FALSE` when the $\lambda$ values are too dense (e.g., assign a large number to `n.lamBeta`/`n.lamTheta`).


## An Application to Structural Neuroimaging and Genetic Data

In this section, we will apply **missoNet** to a relatively high-dimensional dataset -- `bgsmtr_example_data`, as a part of the `bgsmtr` package, obtained from the Alzheimerâ€™s Disease Neuroimaging Initiative (ADNI-1) database.

This example dataset consists of 15 structural neuromaging measures and 486 single nucleotide polymorphisms (SNPs) determined from a sample of 632 subjects. Importantly, the 486 SNPs cover 33 genes deemed associated with Alzheimer's disease. In this case, $\mathbf{X}$ is a 632-by-486 matrix containing minor allele counts (i.e., 0, 1 or 2) for 486 SNPs of the 632 subjects. $\mathbf{Y}$ is a 632-by-15 matrix containing volumetric and cortical thickness measures for 15 regions of interest. For more details, see Greenlaw et al. (2017) [<doi:10.1093/bioinformatics/btx215>](https://academic.oup.com/bioinformatics/article/33/16/2513/3611271?login=false) and [bgsmtr](https://cran.r-project.org/web/packages/bgsmtr/index.html).

First load the dataset and randomly split it into training and validation set. As a demonstration, we only use the SNPs with large variance (i.e., variance in the top 50% among all SNPs, $p$ = 243) as the predictor variables.
```{r, eval = FALSE}
library(bgsmtr, quietly = TRUE)
data(bgsmtr_example_data)

## transpose data matrix to make rows correspond to 
## samples and columns correspond to variables
SNP <- t(bgsmtr_example_data$SNP_data)
BM <- t(bgsmtr_example_data$BrainMeasures)

## unsupervised filtering of top 50% variables
SNP_subset <- apply(SNP, 2, var) > quantile(apply(SNP, 2, var), 0.5)
SNP <- SNP[ ,SNP_subset]

set.seed(123)  ## a random seed for reproducibility
tr <- sample(1:632, 550, replace = FALSE)  ## training set
va <- c(1:632)[-tr]  ## validation set

cat("dim(SNP):", dim(SNP[tr, ]), "
dim(BM):", dim(BM[tr, ]))
```
```{r, echo = FALSE}
cat("dim(SNP): 550 243
dim(BM): 550 15")
```

Since this is a clean dataset, we manually add a little MCAR missing data to the matrix of brain measures with Bernoulli missing probability 0.05:
```{r, eval = FALSE}
## generate the indicator matrix of missingness
M <- matrix(1, nrow(BM), ncol(BM))
NA.pos <- do.call("cbind", lapply(1:ncol(BM), function(x) {rbinom(nrow(BM), size = 1, prob = 0.05) == 1}))
M[NA.pos] <- NA  ## all missing values should be coded as NAs or NaNs

BM.mis = BM * M

cat(BM.mis[1:6, 5:8])
```
```{r, echo = FALSE}
cat("    Left_InfLatVent.adj Left_LatVent.adj Left_EntCtx.adj Left_Fusiform.adj
V1           -473.42061       -4595.2672      0.60678817                NA
V2            100.12989       -9163.7594      0.29868885       0.196951758
V3            373.36131      -10659.8137      0.18835349       0.105863582
V4            876.03547               NA     -1.19832801      -0.233082858
V5           -887.40929       -6789.0606      0.09203347       0.098447465
V6            354.12157               NA     -0.32757347       0.310694508")
```

We start analysis with a large-scope coarse search along the $\lambda$ surface by setting `lamBeta.scale.factor` and `lamTheta.scale.factor` to `3` (to magnify the `lambda.max`), while `lamBeta.min.ratio` and `lamTheta.min.ratio` are given a small value `1e-4` (the default) to ensure that the search domain covers the entire area of all possible solutions. As a rough search, the number of $\lambda$ values usually does not need to be very large, because we only need an approximate range of $\lambda$ where the smallest CV error is most likely to exist.

We train the model without explicitly specifying the working missing probability `rho` (even if we know) and the $\lambda$ values (`lambda.Beta = NULL` and `lambda.Theta = NULL`), which is more in line with the real-world usage. Though many arguments take the defaults, we still try to assign as many commonly used as possible here.

__NOTE__: this is a challenging task because the brain measures have completely different orders of magnitude and do not follow multivariate Gaussian very well, which is very likely to lead to a slow convergence rate and abnormal CV error behaviors (e.g., the optimal area is heavily skewed towards the boundaries of the search domain). Therefore, we adopt a strategy of "coarse + fine" search here to avoid unnecessary over-searches in the areas with obviously excessive errors. Even so, actually not all data needs such tedious two-step processing, we did this simply because of the high computational cost of this dataset that we knew beforehand, and sometimes the `lambda.min` may be not in the "safety" (middle) area depending on the data split.
```{r, eval = FALSE}
cvfit.BM <- cv.missoNet(X = SNP[tr, ], Y = BM.mis[tr, ], kfold = 5, rho = NULL,
                        lambda.Beta = NULL, lambda.Theta = NULL,
                        lamBeta.min.ratio = 1e-4, lamTheta.min.ratio = 1e-4,
                        n.lamBeta = 20, n.lamTheta = 20,
                        lamBeta.scale.factor = 3, lamTheta.scale.factor = 3,
                        standardize = TRUE, standardize.response = TRUE,
                        permute = TRUE, with.seed = 433,
                        parallel = TRUE, cpus = 3, verbose = 0)
```

Plot the heatmap of standardized mean CV error:
```{r, eval = FALSE}
plot(cvfit.BM)
```
```{r, echo = FALSE, fig.align = 'left', out.width = "80%"}
knitr::include_graphics(system.file("extdata", "cvfitBM.png", package = "missoNet"))
```

Now we can adjust the $\lambda$ ranges for a further fine search according to the figure above. We noticed that the current optimal combination of $\lambda_{B}$ and $\lambda_\Theta$ achieving the smallest mean CV error is about (0.24, 0.62), while the largest values of $\lambda_B$ and $\lambda_\Theta$ for coarse searching are around 2.70 and 30.00, respectively. Therefore, we decide to shrink the `lamTheta.scale.factor` from `3` to `0.3` as we believe 3.00 is a more reasonable choice for the upper boundary of $\lambda_\Theta$. For the same reason, the smallest values of $\lambda_B$ and $\lambda_\Theta$ are determined to be 0.027 (= 2.70 * 0.01) and 0.03 (= 3.00 * 0.01) by setting `lamBeta.min.ratio` and `lamTheta.min.ratio` to `0.01`.

Because we are going to do a fine search, the number of $\lambda$ values can be slightly increased. Typically we suggest `n.X = -log10(X.min.ratio) * c`, where `c` $\in$ [10, 20] (`X` is a placeholder for `lamBeta` or `lamTheta`), to achieve a balance between the computational precision and speed.

__TIPS__: More often, real data may produce strange error shapes such that the optimal $\lambda$ pair (`lambda.min`) may appear at the boundaries of the search domain. In this case, users need to adjust the `scale.factor` and/or the `min.ratio` to shift and/or zoom the search ranges of $\lambda_B$ and $\lambda_\Theta$ accordingly.
```{r, eval = FALSE}
## for brevity, arguments take the defaults are not specified explicitly
cvfit2.BM <- cv.missoNet(X = SNP[tr, ], Y = BM.mis[tr, ], kfold = 5,
                         lamBeta.min.ratio = 0.01, lamTheta.min.ratio = 0.01,
                         n.lamBeta = 40, n.lamTheta = 40,
                         lamBeta.scale.factor = 3, lamTheta.scale.factor = 0.3,
                         fit.relax = TRUE, fit.1se = FALSE,
                         permute = TRUE, with.seed = 433,
                         parallel = TRUE, cpus = 3, verbose = 0)
```

Plot the standardized mean CV error of this refitted model, `detailed.axes = FALSE` can prevent axes to get squeezed together:
```{r, eval = FALSE}
cat("`lambda.min`:
  - lambda.Beta:", cvfit2.BM$est.min$lambda.Beta, "
  - lambda.Theta:", cvfit2.BM$est.min$lambda.Theta)

plot(cvfit2.BM, detailed.axes = FALSE)
```
```{r, echo = FALSE}
cat("`lambda.min`:
  - lambda.Beta: 0.198283
  - lambda.Theta: 0.573265")
```
```{r, echo = FALSE, fig.align = 'left', out.width = "80%"}
knitr::include_graphics(system.file("extdata", "cvfit2BM.png", package = "missoNet"))
```

We found the new `lambda.min` = (0.20, 0.57) is actually very close to the one from the coarse search (0.24, 0.62), proving that a single search can provide considerable accuracy most of the time. For the sake of brevity, we shall skip the discussion of `lambda.1se` here. 

As we did before, the estimated parameters $\hat{\mathbf{B}}$ and $\hat{\mathbf{\Theta}}$ at `lambda.min` can be visualized in the form of heatmap:
```{r, eval = FALSE}
## Theta_hat at `lambda.min`
Theta_hat <- cvfit2.BM$est.min$Theta
rownames(Theta_hat) <- colnames(Theta_hat) <- colnames(BM)
col <- circlize::colorRamp2(c(-2, 0, 2), c("blue", "white", "red"))
ht1 <- plot_heatmap(Theta_hat, col = col, legend = TRUE, lgd_name = "values",
                    title = expression(paste(hat(bold(Theta)), " at `lambda.min` (unclustered)")))
plot(ht1)

## Beta_hat at `lambda.min`
Beta_hat <- cvfit2.BM$est.min$Beta
colnames(Beta_hat) <- colnames(BM)
col <- circlize::colorRamp2(c(-0.05, 0, 0.05), c("blue", "white", "red"))
ht2 <- plot_heatmap(Beta_hat, col = col, legend = TRUE, lgd_name = "values",
                    title = expression(paste(hat(bold(Beta)), " at `lambda.min` (unclustered)")))
plot(ht2)

## relaxed network
relax.net <- cvfit2.BM$est.min$relax.net
rownames(relax.net) <- colnames(relax.net) <- colnames(BM)
col <- circlize::colorRamp2(c(-4, 0, 4), c("blue", "white", "red"))
ht3 <- plot_heatmap(relax.net, col = col, legend = TRUE, lgd_name = "values",
                    title = "Relaxed fit of network (unclustered)")
plot(ht3)

## plot the partial correlation matrix of the 15 neuroimaging measures
## must use a complete dataset, here we use the clean data
pcor.mat <- ppcor::pcor(BM)
corrplot::corrplot(pcor.mat$estimate, p.mat = pcor.mat$p.value, 
                   sig.level = 0.01, insig = "blank", 
                   tl.cex = 0.5, tl.col = "black", mar = c(.5, .1, 2, .1),
                   title = "Partial correlation matrix (unclustered)")
```
```{r, echo = FALSE, fig.align = 'left', out.width = "100%"}
# knitr::include_graphics(paste("vignetteFigs", "BMcomb.png", sep = .Platform$file.sep))
knitr::include_graphics(system.file("extdata", "BMcomb.png", package = "missoNet"))
```

The conditional network structure can be represented by an undirected graph $\mathcal{G}$, consisting of a set of vertices $V$ and a set of edges $E$, where an edge connects a pair of variables if and only if they are conditionally dependent. One important feature of **missoNet** is that the program supports fitting a relaxed conditional graphical model upon response variables (by setting `fit.relax = TRUE`). The active edges (non-zero off-diagonal elements) of $\hat{\mathbf{\Theta}}$ will be re-estimated without penalization (`lambda.Theta = 0`), such a debiased estimate of the network structure would be useful for exploring the conditional inter-dependencies among traits of interest.

The two figures in the second row above show the relaxed conditional network and partial correlation matrix of the 15 neuroimaging measures. As will be readily seen that **missoNet** sharply reduces potential noisy dependencies compared with the naive partial correlation analysis (insignificant correlations $p>0.01$ were not plotted), by regressing on SNPs and adjusting for the genetic effects, which helps us better focus on those truly important, and non-genetic induced connections. Moreover, we have to emphasize that unlike the traditional partial correlation analysis requesting complete datasets (although pair-wise complete cases can be used, the results tend to be biased), **missoNet** substantially reduces the difficulty of data acquisition and processing by allowing for missing values.

Clustering the coefficient matrix $\hat{\mathbf{B}}$ by rows (SNPs) in an unsupervised manner:
```{r, eval = FALSE}
colnames(Beta_hat) <- colnames(BM)  ## brain measure names
rownames(Beta_hat) <- colnames(SNP)  ## SNP names
Gene_groups <- bgsmtr_example_data$SNP_groups[SNP_subset]

ComplexHeatmap::pheatmap(Beta_hat,
                         annotation_row = data.frame("Gene" = Gene_groups, row.names = rownames(Beta_hat)),
                         color = circlize::colorRamp2(c(-0.05,0,0.05), c("blue","white","red")), show_rownames = FALSE,
                         border_color = NA, border = TRUE, name = "values", cluster_rows = TRUE, cluster_cols = FALSE,
                         column_title = expression(paste(hat(bold(Beta)), " at `lambda.min` (unclustered)")))
```
```{r, eval = FALSE, include = FALSE}
set.seed(34)  # seed for legend colors
ComplexHeatmap::pheatmap(cvfit2.BM$est.min$Beta,
                         annotation_row = data.frame("Gene" = cvfit2.BM$Gene, row.names = rownames(cvfit2.BM$est.min$Beta)),
                         color = circlize::colorRamp2(c(-0.05,0,0.05), c("blue","white","red")), show_rownames = FALSE,
                         border_color = NA, border = TRUE, name = "values", cluster_rows = TRUE, cluster_cols = FALSE,
                         column_title = expression(paste(hat(bold(Beta)), " at `lambda.min` (clustered)")))
```
```{r, echo = FALSE, fig.align = 'left', out.width = "80%"}
# knitr::include_graphics(paste("vignetteFigs", "BMcomb.png", sep = .Platform$file.sep))
knitr::include_graphics(system.file("extdata", "cvfit2BMclustB.png", package = "missoNet"))
```

The figure depicts a very large proportion of coefficients with zero values, and there is an obvious clustering of the contributing SNPs with respect to the genes (see row-wise annotation), implying potential pathways between certain SNPs and changes in the local structures of brain.

Finally, we end this section by training the model on the clean data:
```{r, eval = FALSE}
## all arguments are kept unchanged
## only BM.mis -> BM
cvfit3.BM <- cv.missoNet(X = SNP[tr, ], Y = BM[tr, ], kfold = 5,
                         lamBeta.min.ratio = 0.01, lamTheta.min.ratio = 0.01,
                         n.lamBeta = 40, n.lamTheta = 40,
                         lamBeta.scale.factor = 3, lamTheta.scale.factor = 0.3,
                         fit.relax = FALSE, fit.1se = FALSE,
                         permute = TRUE, with.seed = 433,
                         parallel = TRUE, cpus = 3, verbose = 0)
```

Then we can assess the prediction performance of the models trained using the corrupted vs. clean data. As expected, there is no obvious difference between the two models with regard to the mean absolute prediction error (MAE) on the validation set, since we generated missing data simply following MCAR.
```{r, eval = FALSE}
## predictions on the validation set
newy.mis <- predict(cvfit2.BM, newx = SNP[va, ], s = "lambda.min")
newy <- predict(cvfit3.BM, newx = SNP[va, ], s = "lambda.min")

cat("MAE on the validation set:
  - Model II (corrupted):", mean(abs(BM[va, ] - newy.mis)), "
  - Model III (clean):", mean(abs(BM[va, ] - newy)))
```
```{r, echo = FALSE}
cat("MAE on the validation set:
  - Model II (corrupted): 2543.588
  - Model III (clean): 2543.591")
```


## References